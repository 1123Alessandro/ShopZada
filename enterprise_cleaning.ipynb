{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding product_list.xlsx ...\n",
      "Adding user_credit_card.pickle ...\n",
      "Adding user_data.json ...\n",
      "Adding user_job.csv ...\n",
      "Adding merchant_data.html ...\n",
      "Adding order_with_merchant_data1.parquet ...\n",
      "Adding order_with_merchant_data2.parquet ...\n",
      "Adding order_with_merchant_data3.csv ...\n",
      "Adding staff_data.html ...\n",
      "Adding campaign_data.csv ...\n",
      "Adding transactional_campaign_data.csv ...\n",
      "Adding line_item_data_prices1.csv ...\n",
      "Adding line_item_data_prices2.csv ...\n",
      "Adding line_item_data_prices3.parquet ...\n",
      "Adding line_item_data_products1.csv ...\n",
      "Adding line_item_data_products2.csv ...\n",
      "Adding line_item_data_products3.parquet ...\n",
      "Adding order_data_20200101-20200701.parquet ...\n",
      "Adding order_data_20200701-20211001.pickle ...\n",
      "Adding order_data_20211001-20220101.csv ...\n",
      "Adding order_data_20220101-20221201.xlsx ...\n",
      "Adding order_data_20221201-20230601.json ...\n",
      "Adding order_data_20230601-20240101.html ...\n",
      "Adding order_delays.html ...\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Read Complete\n",
      "Access the DataFrame of each file using the `all` dictionary\n",
      "e.g. all[\"filename\"]\n",
      "\n",
      "below are the available keys for access:\n",
      "1:\tproduct_list.xlsx\n",
      "2:\tuser_credit_card.pickle\n",
      "3:\tuser_data.json\n",
      "4:\tuser_job.csv\n",
      "5:\tmerchant_data.html\n",
      "6:\torder_with_merchant_data1.parquet\n",
      "7:\torder_with_merchant_data2.parquet\n",
      "8:\torder_with_merchant_data3.csv\n",
      "9:\tstaff_data.html\n",
      "10:\tcampaign_data.csv\n",
      "11:\ttransactional_campaign_data.csv\n",
      "12:\tline_item_data_prices1.csv\n",
      "13:\tline_item_data_prices2.csv\n",
      "14:\tline_item_data_prices3.parquet\n",
      "15:\tline_item_data_products1.csv\n",
      "16:\tline_item_data_products2.csv\n",
      "17:\tline_item_data_products3.parquet\n",
      "18:\torder_data_20200101-20200701.parquet\n",
      "19:\torder_data_20200701-20211001.pickle\n",
      "20:\torder_data_20211001-20220101.csv\n",
      "21:\torder_data_20220101-20221201.xlsx\n",
      "22:\torder_data_20221201-20230601.json\n",
      "23:\torder_data_20230601-20240101.html\n",
      "24:\torder_delays.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scripts.load import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning order data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 3)\n",
      "(200000, 3)\n",
      "(200000, 4)\n"
     ]
    }
   ],
   "source": [
    "##Cleaning order data\n",
    "df_order1 = all['order_with_merchant_data1.parquet']\n",
    "df_order2 = all['order_with_merchant_data2.parquet']\n",
    "df_order3 = all['order_with_merchant_data3.csv']\n",
    "\n",
    "print(df_order1.shape)\n",
    "print(df_order2.shape)\n",
    "print(df_order3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop unnecessary indexing\n",
    "df_order3 = df_order3.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine all order files\n",
    "df_orders = pd.concat([df_order1, df_order2, df_order3], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500000, 3)\n",
      "order_id       500000\n",
      "merchant_id      4812\n",
      "staff_id         4794\n",
      "dtype: int64\n",
      "order_id       0\n",
      "merchant_id    0\n",
      "staff_id       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#order_with_merchant_data 1-3 cleaned completely, to check:\n",
    "print(df_orders.shape)\n",
    "print(df_orders.nunique())\n",
    "print(df_orders.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning merchant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 9)\n",
      "Unnamed: 0        5000\n",
      "merchant_id       4812\n",
      "creation_date     5000\n",
      "name               529\n",
      "street            5000\n",
      "state               50\n",
      "city                98\n",
      "country            249\n",
      "contact_number    5000\n",
      "dtype: int64\n",
      "Unnamed: 0        0\n",
      "merchant_id       0\n",
      "creation_date     0\n",
      "name              0\n",
      "street            0\n",
      "state             0\n",
      "city              0\n",
      "country           0\n",
      "contact_number    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cleaning merchant data\n",
    "df_merchant_data = all['merchant_data.html'][0]\n",
    "\n",
    "print(df_merchant_data.shape)\n",
    "print(df_merchant_data.nunique())\n",
    "print(df_merchant_data.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Unnamed: 0 column\n",
    "df_merchant_data = df_merchant_data.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplicates based on merchant_id, keep first instance based on creation_date\n",
    "df_merchant_data = df_merchant_data.sort_values(by=['merchant_id', 'creation_date']).drop_duplicates(subset=['merchant_id'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize contact_number\n",
    "df_merchant_data['contact_number'] = df_merchant_data['contact_number'].str.replace('\\.', '-', regex=True)\n",
    "df_merchant_data['contact_number'] = df_merchant_data['contact_number'].str.replace('[^0-9+()-]', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Title case for address except for country\n",
    "df_merchant_data['street'] = df_merchant_data['street'].str.title()\n",
    "df_merchant_data['state'] = df_merchant_data['state'].str.title()\n",
    "df_merchant_data['city'] = df_merchant_data['city'].str.title()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4812, 8)\n",
      "merchant_id       4812\n",
      "creation_date     4812\n",
      "name               529\n",
      "street            4812\n",
      "state               50\n",
      "city                98\n",
      "country            249\n",
      "contact_number    4812\n",
      "dtype: int64\n",
      "merchant_id       0\n",
      "creation_date     0\n",
      "name              0\n",
      "street            0\n",
      "state             0\n",
      "city              0\n",
      "country           0\n",
      "contact_number    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#merchant data cleaned completely, to check:\n",
    "print(df_merchant_data.shape)\n",
    "print(df_merchant_data.nunique())\n",
    "print(df_merchant_data.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning staff data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 10)\n",
      "Unnamed: 0        5000\n",
      "staff_id          4794\n",
      "name              4983\n",
      "job_level            3\n",
      "street            5000\n",
      "state               50\n",
      "city                98\n",
      "country            249\n",
      "contact_number    5000\n",
      "creation_date     5000\n",
      "dtype: int64\n",
      "Unnamed: 0        0\n",
      "staff_id          0\n",
      "name              0\n",
      "job_level         0\n",
      "street            0\n",
      "state             0\n",
      "city              0\n",
      "country           0\n",
      "contact_number    0\n",
      "creation_date     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cleaning staff data\n",
    "df_staff_data = all['staff_data.html'][0]\n",
    "\n",
    "print(df_staff_data.shape)\n",
    "print(df_staff_data.nunique())\n",
    "print(df_staff_data.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Unnamed: 0 column\n",
    "df_staff_data = df_staff_data.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplicates based on staff_id, keep first instance based on creation_date\n",
    "df_staff_data = df_staff_data.sort_values(by=['staff_id', 'creation_date']).drop_duplicates(subset=['staff_id'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize contact_number\n",
    "df_staff_data['contact_number'] = df_staff_data['contact_number'].str.replace('\\.', '-', regex=True)\n",
    "df_staff_data['contact_number'] = df_staff_data['contact_number'].str.replace('[^0-9+()-]', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize strings, use title case except for country\n",
    "df_staff_data['name'] = df_staff_data['name'].str.title()\n",
    "df_staff_data['job_level'] = df_staff_data['job_level'].str.title()\n",
    "df_staff_data['street'] = df_staff_data['street'].str.title()\n",
    "df_staff_data['state'] = df_staff_data['state'].str.title()\n",
    "df_staff_data['city'] = df_staff_data['city'].str.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4794, 9)\n",
      "staff_id          4794\n",
      "name              4779\n",
      "job_level            3\n",
      "street            4794\n",
      "state               50\n",
      "city                98\n",
      "country            249\n",
      "contact_number    4794\n",
      "creation_date     4794\n",
      "dtype: int64\n",
      "staff_id          0\n",
      "name              0\n",
      "job_level         0\n",
      "street            0\n",
      "state             0\n",
      "city              0\n",
      "country           0\n",
      "contact_number    0\n",
      "creation_date     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#merchant data cleaned completely, to check:\n",
    "print(df_staff_data.shape)\n",
    "print(df_staff_data.nunique())\n",
    "print(df_staff_data.isna().sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
